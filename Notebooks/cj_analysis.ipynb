{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca445310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import blocksci\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "\n",
    "parser_data_directory = Path(\"/mnt/anal/config.json\")\n",
    "cluster_directory = Path(\"/mnt/anal/cluster/\")\n",
    "dumplings_directory = Path(\"/mnt/dumplings/\")\n",
    "\n",
    "chain = blocksci.Blockchain(str(parser_data_directory))\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "def get_block_height_for_date(date: str) -> int:\n",
    "    return chain.range(date)[0].height\n",
    "\n",
    "def get_block_height_range(start: str, end: str) -> Tuple[int, int]:\n",
    "    return get_block_height_for_date(start), get_block_height_for_date(end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77f0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "wasabi2_events_file = dumplings_directory / \"wasabi2_events.json\"\n",
    "# wasabi2_txs_file = dumplings_directory / \"wasabi2_txs.json\"\n",
    "wasabi_events_file = dumplings_directory / \"wasabi1_events.json\"\n",
    "# wasabi_txs_file = dumplings_directory / \"wasabi_txs.json\"\n",
    "whirlpool_events_file = dumplings_directory / \"whirlpool_events.json\"\n",
    "# whirlpool_txs_file = dumplings_directory / \"whirlpool_txs.json\"\n",
    "\n",
    "with open(wasabi2_events_file) as f:\n",
    "    wasabi2_events = json.load(f)\n",
    "\n",
    "# with open(wasabi2_txs_file) as f:\n",
    "#     wasabi2_txs = json.load(f)\n",
    "\n",
    "with open(wasabi_events_file) as f:\n",
    "    wasabi_events = json.load(f)\n",
    "\n",
    "# with open(wasabi_txs_file) as f:\n",
    "#     wasabi_txs = json.load(f)\n",
    "\n",
    "with open(whirlpool_events_file) as f:\n",
    "    whirlpool_events = json.load(f)\n",
    "\n",
    "# with open(whirlpool_txs_file) as f:\n",
    "#     whirlpool_txs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f7c29771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "https://mempool.space/tx/57a8ea3ba1568fed4d9f7d7b3b84cdec552d9c49d4849bebf77a1053c180d0d1\n",
      "76.3299438 BTC\n",
      "https://mempool.space/tx/32cad2742cc904e74769f637cb63e8c94dc5dbf0f4d0c18b6cad679465c458a8\n",
      "67.0 BTC\n",
      "https://mempool.space/tx/d463b35b3d18dda4e59f432728c7a365eaefd50b24a6596ab42a077868e9d7e5\n",
      "62.38784013 BTC\n",
      "https://mempool.space/tx/8f59577b2dfa88e7d7fdd206a17618893db7559007a15658872b665bc16417c5\n",
      "62.21724419 BTC\n",
      "https://mempool.space/tx/e7c672f95690a3495fe50d1688311b4908e913790ec8f08dd6b76a3f401a3982\n",
      "44.98111892 BTC\n",
      "https://mempool.space/tx/d2183b4d4838f0a767f8c82a449e735d344fdc1fb27109a0a3a0c9317f90c1d6\n",
      "36.24910764 BTC\n",
      "https://mempool.space/tx/c9d870f7b73b1f10d217d625208968c2690df864c7e39e69746ad538cd25041c\n",
      "33.054 BTC\n",
      "https://mempool.space/tx/aaff209fcdef69736ac59bc6d82637cf5c8d1f24f31561398b424284203de199\n",
      "29.9 BTC\n",
      "https://mempool.space/tx/217240acda764b244aa4878f9f3c7e0934997fef0c9c82b00a787edccbd550e9\n",
      "29.525 BTC\n",
      "https://mempool.space/tx/deccc09df7f3549300b13b36d4b26f118b0573b0b8afd809d8d20167205c3ce2\n",
      "26.95 BTC\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from_top = list(\n",
    "    # sort by input value\n",
    "    sorted(\n",
    "        # get newer than date\n",
    "        filter(\n",
    "            lambda x: x.block_time > datetime(2024, 3, 1), \n",
    "            # turn into blocksci objects\n",
    "            map(\n",
    "                lambda x: chain.tx_with_hash(x), set(consolidations)\n",
    "            )\n",
    "        ),\n",
    "        key=lambda x: -x.input_value\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "print(\"57a8ea3ba1568fed4d9f7d7b3b84cdec552d9c49d4849bebf77a1053c180d0d1\" in consolidations)\n",
    "print(\"d463b35b3d18dda4e59f432728c7a365eaefd50b24a6596ab42a077868e9d7e5\" in consolidations)\n",
    "print(\"8f59577b2dfa88e7d7fdd206a17618893db7559007a15658872b665bc16417c5\" in consolidations)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"https://mempool.space/tx/{from_top[i].hash}\")\n",
    "    print(from_top[i].input_value / 100000000, \"BTC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "038a1564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 188 ms, total: 1min 32s\n",
      "Wall time: 645 ms\n"
     ]
    }
   ],
   "source": [
    "first_in_pool_denominations = {}\n",
    "\n",
    "tx: blocksci.Tx\n",
    "    \n",
    "for tx in filtered_whilrpool_coinjoins:\n",
    "    pool_size = str(min(map(lambda t: t.value, tx.inputs)))\n",
    "    if pool_size not in first_in_pool_denominations:\n",
    "        first_in_pool_denominations[pool_size] = [tx, 0]\n",
    "    else:\n",
    "        first_in_pool_denominations[pool_size][1] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61e895d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pool: 0.05 BTC, starts at 2019-04-19 16:43:01\n",
      "Count of txs: 112135\n",
      "Pool: 0.01 BTC, starts at 2019-05-23 19:09:51\n",
      "Count of txs: 205644\n",
      "Pool: 0.5 BTC, starts at 2019-08-03 05:53:42\n",
      "Count of txs: 30325\n",
      "Pool: 0.001 BTC, starts at 2021-03-05 22:56:29\n",
      "Count of txs: 193011\n"
     ]
    }
   ],
   "source": [
    "for size, (tx, count) in first_in_pool_denominations.items():\n",
    "    print(f\"Pool: {float(size) / 100000000} BTC, starts at {tx.block_time}\")\n",
    "#     print(f\"\\thttps://mempool.space/tx/{tx.hash}\")\n",
    "    print(f\"Count of txs: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eca3c7e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 435 ms, total: 1min 37s\n",
      "Wall time: 658 ms\n"
     ]
    }
   ],
   "source": [
    "%time filtered_ww1_events = chain.filter_coinjoin_txes(0, len(chain), \"ww2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f1089b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "For wasabi1:\n",
      "Only in dumplings: 0, only in blocksci: 107, in both: 34955\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for events in ((filtered_ww1_events, \"wasabi1\", wasabi_events),):\n",
    "    s, name, e = events\n",
    "    d = set(map(lambda x: str(x.hash), s))\n",
    "    res = {'only_in_dumplings': [], 'only_in_blocksci': [], 'in_both': []}\n",
    "\n",
    "    for tx in s:\n",
    "        if str(tx.hash) in d and str(tx.hash) in e:\n",
    "            res['in_both'].append(tx)\n",
    "        elif str(tx.hash) in d:\n",
    "            res['only_in_blocksci'].append(tx)\n",
    "\n",
    "    for tx in e.keys():\n",
    "        if tx not in d:\n",
    "            res['only_in_dumplings'].append(tx)\n",
    "\n",
    "    print(f\"\\nFor {name}:\\nOnly in dumplings: {len(res['only_in_dumplings'])}, only in blocksci: {len(res['only_in_blocksci'])}, in both: {len(res['in_both'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ed3c9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 11s, sys: 60 ms, total: 1min 11s\n",
      "Wall time: 547 ms\n",
      "<class 'dict'> 9\n",
      "Pay to witness pubkey hash: 272536756\n",
      "Pay to witness unknown: 197219154\n",
      "Pay to script hash: 103787612\n",
      "Pay to pubkey hash: 85680424\n",
      "Null data: 42567683\n",
      "Pay to witness script hash: 15242387\n",
      "Multisig: 1256112\n",
      "Nonstandard: 11065\n",
      "Pay to pubkey: 289\n"
     ]
    }
   ],
   "source": [
    "%time result = chain.get_address_types(get_block_height_for_date('2023-01-01'), len(chain))\n",
    "\n",
    "print(type(result), len(result))\n",
    "\n",
    "for k, v in sorted(result.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210d4e4",
   "metadata": {},
   "source": [
    "# Basic analysis\n",
    "Here are some basic analyses for the coinjoins just to make sure nothign fishy is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876a2764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_max(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return (tx.input_count, tx.output_count)\n",
    "\n",
    "def find_number_of_unique_txs(tx: blocksci.Tx) -> Tuple[int, int]:\n",
    "    return len(set([x.value for x in tx.inputs])), len(set([x.value for x in tx.outputs]))\n",
    "    \n",
    "def find_timestamped_txes(tx: blocksci.Tx) -> bool:\n",
    "    return tx.time_seen is not None and tx.timestamp_seen is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e480ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_counts = chain.map_spliterator(map_func=find_number_of_unique_txs, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3740181",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time timestamped = chain.map_spliterator(map_func=find_timestamped_txes, keys=list(map(lambda t: str(t.hash), chain.blocks.txes)), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e295e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import median\n",
    "\n",
    "flattened_unique_inputs = [x[0] for y in unique_counts for x in y]\n",
    "flattened_unique_outputs = [x[1] for y in unique_counts for x in y]\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(f\"min: {min(flattened_unique_inputs)}, max: {max(flattened_unique_inputs)}, median: {median(flattened_unique_inputs)}\")\n",
    "\n",
    "print(\"outputs:\")\n",
    "print(f\"min: {min(flattened_unique_outputs)}, max: {max(flattened_unique_outputs)}, median: {median(flattened_unique_outputs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxes = chain.map_spliterator(map_func=find_min_max, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f809b307",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp, out = minmaxes[0][0]\n",
    "\n",
    "for x in minmaxes:\n",
    "    for input1, output1 in x:\n",
    "        if input1 > inp:\n",
    "            inp = input1\n",
    "        if output1 > out:\n",
    "            out = output1\n",
    "            \n",
    "print(inp, out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42483ad5",
   "metadata": {},
   "source": [
    "# Remix analysis\n",
    "\n",
    "Here we have the map functions for different remix analyses. The functions should have the following interface\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> T`\n",
    "where `T` is the common result type. The result will be added to the list of results of each worker.\n",
    "\n",
    "`kwargs` are the arguments passed to each worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac4427",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_remixes_within_one_hop(tx: blocksci.Tx, **kwargs):\n",
    "    \"\"\"Pass in `events` as kwarg. Will check whether there is an output of `tx` in `events`.\n",
    "    We can count this as 'remix' transaction.\n",
    "    \"\"\"\n",
    "    cj_events = kwargs['events']\n",
    "    result = (tx.hash, tx.output_count, [])\n",
    "    for c, i in enumerate(tx.outputs):\n",
    "        if not i.is_spent:\n",
    "            continue\n",
    "        \n",
    "        if str(i.spending_tx.hash) in cj_events:\n",
    "            result[2].append((i.index, True))\n",
    "            \n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bb68fa",
   "metadata": {},
   "source": [
    "### Results processing\n",
    "\n",
    "We take the outputs of the above functions and compute various statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b80083",
   "metadata": {},
   "outputs": [],
   "source": [
    "wasabi2_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi2_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi2_events.keys())\n",
    "wasabi_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(wasabi_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=64, events=wasabi_events.keys())\n",
    "whirlpool_results = chain.map_spliterator(map_func=find_remixes_within_one_hop, keys=list(whirlpool_txs[\"coinjoins\"].keys()), data_directory=str(parser_data_directory), workers=128, events=whirlpool_events.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_remix_stats(results, events, key):\n",
    "    stats_computed = {\"remix\": 0, \"left\": 0}\n",
    "    \n",
    "    for one in results:\n",
    "        for txid, all_outputs, actual in one:\n",
    "            stats_computed[\"remix\"] += len(actual)\n",
    "            stats_computed[\"left\"] += all_outputs - len(actual)\n",
    "    \n",
    "    print(key)\n",
    "    print(stats_computed)\n",
    "\n",
    "    dumplings_stats = {\"dumplings_remix\": 0}\n",
    "\n",
    "    for tx in events.values():\n",
    "        for out in tx[\"outputs\"].values():\n",
    "            if out[\"mix_event_type\"] == \"MIX_REMIX\":\n",
    "                dumplings_stats[\"dumplings_remix\"] += 1\n",
    "\n",
    "    print(dumplings_stats)\n",
    "    print(f\"dumplings - computed: {dumplings_stats['dumplings_remix'] - stats_computed['remix']}\\n\")\n",
    "\n",
    "compute_remix_stats(wasabi2_results, wasabi2_txs[\"coinjoins\"], \"wasabi2 1 hop\")\n",
    "compute_remix_stats(wasabi_results, wasabi_txs[\"coinjoins\"], \"wasabi 1 hop\")\n",
    "compute_remix_stats(whirlpool_results, whirlpool_txs[\"coinjoins\"], \"whirlpool 1 hop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02838a96",
   "metadata": {},
   "source": [
    "# Consolidation analysis\n",
    "\n",
    "The functions for consolidation analysis are here. The interface is still the same:\n",
    "`map_func(tx: blocksci.Tx, **kwargs) -> list[T]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf071fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def find_outputs_one_hop(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        output_spent_in = str(output.spending_tx.hash)\n",
    "        if output_spent_in not in found:\n",
    "            found[output_spent_in] = 0\n",
    "        found[output_spent_in] += 1\n",
    "\n",
    "\n",
    "\n",
    "def find_consolidation(tx: blocksci.Tx, **kwargs) -> List[Tuple[str, Dict[str, List[str]]]]:\n",
    "    found_for_tx = {}\n",
    "    find_outputs_one_hop(tx, found_for_tx)\n",
    "    return str(tx.hash), found_for_tx\n",
    "\n",
    "\n",
    "def has_tx_output_in_coinjoin_events(tx: blocksci.Tx, **kwargs) -> bool:\n",
    "    cj_events = kwargs[\"events\"]\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if str(output.spending_tx.hash) in cj_events:\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def find_outputs_two_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                continue\n",
    "\n",
    "\n",
    "def find_outputs_three_hops(tx: blocksci.Tx, found: Dict[str, List[str]]):\n",
    "    for output in tx.outputs:\n",
    "        if not output.is_spent:\n",
    "            continue\n",
    "            \n",
    "        found_match = False\n",
    "\n",
    "        if output.spending_tx.output_count < 2:\n",
    "            output_spent_in = str(output.spending_tx.hash)\n",
    "            if output_spent_in not in found:\n",
    "                found[output_spent_in] = 0\n",
    "            found[output_spent_in] += 1\n",
    "            continue\n",
    "\n",
    "        for output2 in output.spending_tx.outputs:\n",
    "            if found_match:\n",
    "                break\n",
    "            if not output2.is_spent:\n",
    "                continue\n",
    "                \n",
    "            if output2.spending_tx.output_count < 2:\n",
    "                output_spent_in = str(output2.spending_tx.hash)\n",
    "                if output_spent_in not in found:\n",
    "                    found[output_spent_in] = 0\n",
    "                found[output_spent_in] += 1\n",
    "                found_match = True\n",
    "                break\n",
    "            \n",
    "            for output3 in output2.spending_tx.outputs:\n",
    "                if not output3.is_spent:\n",
    "                    continue\n",
    "\n",
    "                if output3.spending_tx.output_count < 2:\n",
    "                    output_spent_in = str(output3.spending_tx.hash)\n",
    "                    if output_spent_in not in found:\n",
    "                        found[output_spent_in] = 0\n",
    "                    found[output_spent_in] += 1\n",
    "                    found_match = True\n",
    "                    break\n",
    "                    \n",
    "def find_consolidation_three_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_three_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "def find_consolidation_two_hops(tx: blocksci.Tx):\n",
    "    found_for_tx = {}\n",
    "    find_outputs_two_hops(tx, found_for_tx)\n",
    "    return (str(tx.hash), found_for_tx)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32c2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi2_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory), workers=64)\n",
    "# %time wasabi_consolidation_three_hop = chain.map_spliterator(map_func=find_consolidation_three_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069d7bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time wasabi_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(wasabi_events.keys()), data_directory=str(parser_data_directory), workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8ed9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time whirlpool_consolidation_two_hop = chain.map_spliterator(map_func=find_consolidation_two_hops, keys=list(whirlpool_events.keys()), data_directory=str(parser_data_directory), workers=128)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea7b7b",
   "metadata": {},
   "source": [
    "### Consolidation analyses\n",
    "\n",
    "Following are the functions computing the actual result analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57a7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def compute_consolidated_txes_in_one_hop(consolidated):\n",
    "    total_outgoing = 0\n",
    "    counts_of_consolidated_txes_in_one_hop = defaultdict(int)\n",
    "    for root_tx, dct in consolidated:\n",
    "            for outbound_tx, count in dct.items():\n",
    "                if outbound_tx in wasabi2_events or outbound_tx in wasabi_events or outbound_tx in whirlpool_events:\n",
    "                    continue\n",
    "\n",
    "                counts_of_consolidated_txes_in_one_hop[count] += 1\n",
    "    return counts_of_consolidated_txes_in_one_hop\n",
    "\n",
    "\n",
    "# w2_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi2_consolidation)\n",
    "# w_consolidation_1hop = compute_consolidated_txes_in_one_hop(wasabi_consolidation)\n",
    "# whirl_consolidation_1hop = compute_consolidated_txes_in_one_hop(whirlpool_consolidation)\n",
    "\n",
    "def plot_barplot_from_dictionary(dct):\n",
    "    plt.bar(range(len(dct)), list(dct.values()), align='center')\n",
    "    plt.xticks(range(len(dct)), list(dct.keys()))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def make_graph(data, name):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize=(8, 9))\n",
    "\n",
    "    # Plot the data for 2-8 on the first subplot\n",
    "    ax1.bar(list(range(2, 9)), [data[key] for key in range(2, 9)])\n",
    "    ax1.set_xlabel('X-axis')\n",
    "    ax1.set_ylabel('Y-axis (2-8)')\n",
    "    ax1.set_title(f'{name}: Bar Plot (2-8)')\n",
    "\n",
    "    # Plot the data for 9-15 on the second subplot\n",
    "    ax2.bar(list(range(9, 16)), [data[key] for key in range(9, 16)])\n",
    "    ax2.set_xlabel('X-axis')\n",
    "    ax2.set_ylabel('Y-axis (9-15)')\n",
    "    ax2.set_title(f'{name}: Bar Plot (9-15)')\n",
    "\n",
    "    ax3.bar(list(range(16, 23)), [data[key] for key in range(16, 23)])\n",
    "    ax3.set_xlabel('X-axis')\n",
    "    ax3.set_ylabel('Y-axis (16-22)')\n",
    "    ax3.set_title(f'{name}: Bar Plot (16-22)')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# print(f\"w2 unpaired: {w2_consolidation_1hop[1]}\")\n",
    "# print(f\"w unpaired: {w_consolidation_1hop[1]}\")\n",
    "# print(f\"whirlpool unpaired: {whirl_consolidation_1hop[1]}\")\n",
    "\n",
    "# make_graph(w2_consolidation_1hop, \"wasabi2\")\n",
    "# make_graph(w_consolidation_1hop, \"wasabi\")\n",
    "# make_graph(whirl_consolidation_1hop, \"whirlpool\")\n",
    "\n",
    "\n",
    "\n",
    "# w2_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi2_consolidation_two_hop)\n",
    "# w_2hop_consolidated = compute_consolidated_txes_in_one_hop(wasabi_consolidation_two_hop)\n",
    "# wh_2hop_consolidated = compute_consolidated_txes_in_one_hop(whirlpool_consolidation_two_hop)\n",
    "\n",
    "\n",
    "# make_graph(w2_2hop_consolidated, \"two hop consolidation to one output wasabi2\")\n",
    "# make_graph(w_2hop_consolidated, \"two hop consolidation to one output wasabi\")\n",
    "# make_graph(wh_2hop_consolidated, \"two hop consolidation to one output whirlpool\")\n",
    "\n",
    "print(len(res_w2))\n",
    "w2_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w2)\n",
    "w_consolidation_3hop = compute_consolidated_txes_in_one_hop(res_w1)\n",
    "\n",
    "\n",
    "print(f\"w2 unpaired: {w2_consolidation_3hop[1]}\")\n",
    "print(f\"w unpaired: {w_consolidation_3hop[1]}\")\n",
    "\n",
    "\n",
    "make_graph(w2_consolidation_3hop, \"wasabi2\")\n",
    "make_graph(w_consolidation_3hop, \"wasabi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5e2939",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "in_tx = '0002eb55bd780c4fc10f212bb686a0a9426ef11d611827605ba8f82db30bcbbc'\n",
    "consolidated_tx = '0abebd6704fcd886b1e74815ce05a24a11aa2d0e543729d6dbd18629c72874a7'\n",
    "\n",
    "in_tx = chain.tx_with_hash(in_tx)\n",
    "consolidated_tx = chain.tx_with_hash(consolidated_tx) \n",
    "\n",
    "print(consolidated_tx.output_count)\n",
    "\n",
    "\n",
    "from typing import Optional, Set\n",
    "\n",
    "consolidated_txs_lost = list(find_consolidation_two_hops(in_tx))[1]\n",
    "\n",
    "# print(consolidated_txs)\n",
    "consolidated_txs = [x for x, y in consolidated_txs_lost.items() if y < 10]\n",
    "print(len(consolidated_txs), consolidated_txs_lost[consolidated_txs[0]])\n",
    "\n",
    "def subset_sum_rec(nums: List[int], total: int, start: int, memo: Dict[Tuple[int, int], Optional[Set[int]]]) -> Optional[Set[int]]:\n",
    "    if total == 0:\n",
    "        return set()\n",
    "\n",
    "    if start == len(nums):\n",
    "        return None\n",
    "\n",
    "    key = (start, total)\n",
    "    if key in memo:\n",
    "        return memo[key]\n",
    "\n",
    "    num = nums[start]\n",
    "    if num.value > total:\n",
    "        memo[key] = None\n",
    "        return None\n",
    "\n",
    "    result = subset_sum_rec(nums, total - num.value, start + 1, memo)\n",
    "    if result is not None:\n",
    "        result.add(num)\n",
    "        memo[key] = result\n",
    "        return result\n",
    "\n",
    "    result = subset_sum_rec(nums, total, start + 1, memo)\n",
    "    memo[key] = result\n",
    "    return result\n",
    "\n",
    "def find_sum_candidates(tx, inputs, output_value):\n",
    "    memo = {}\n",
    "    sorted_inputs = sorted(filter(lambda y: y.value <= output_value, inputs), key=lambda x: x.value)\n",
    "    return subset_sum_rec(sorted_inputs, output_value, 0, memo)\n",
    "\n",
    "a = set()\n",
    "for i in range(3):\n",
    "    inputs = set(in_tx.inputs) - a\n",
    "    value = chain.tx_with_hash(consolidated_txs[i]).outputs[0].value\n",
    "    r = find_sum_candidates(in_tx, inputs, value)\n",
    "    if r:\n",
    "        r = a\n",
    "\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fecef9",
   "metadata": {},
   "source": [
    "# embrace VUT\n",
    "\n",
    "Let's try to use some analyses with [coinomon](https://coinomon.bazar.nesad.fit.vutbr.cz/#/Authentication/login).\n",
    "\n",
    "- get all coinjoins in one month (say Feb 23)\n",
    "- get all output txs from them (with one output)\n",
    "- pick one randomly and get some data about them from coinomon\n",
    "- ???\n",
    "- profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d258f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CoinomonClient:\n",
    "    def __init__(self, token: str) -> None:\n",
    "        self.crypto = \"BTC\"\n",
    "        self.headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "        self.base_url = \"https://coinomon.bazar.nesad.fit.vutbr.cz/\"\n",
    "    \n",
    "    def get_address_info(self, address: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"{self.base_url}jwt/v1/{self.crypto}/cryptoaddress/{address}/summary\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response.text))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    \n",
    "    def get_cluster_info(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "    def get_cluster_addresses(self, cluster_id: str) -> Dict[str, Any]:\n",
    "        response = requests.get(f\"/jwt/v1/{self.crypto}/cryptocluster/{cluster_id}/addresses\", headers=self.headers)\n",
    "        if response.status_code >= 400:\n",
    "            print(str(response))\n",
    "            return {}\n",
    "        \n",
    "        return response.json()\n",
    "    \n",
    "token = \"insert here\"\n",
    "\n",
    "coinomon_client = CoinomonClient(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e622f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 24s, sys: 5min, total: 12min 24s\n",
      "Wall time: 5.04 s\n"
     ]
    }
   ],
   "source": [
    "# start, end = get_block_height_range('2023-02-01', '2023-02-28')\n",
    "# %time res = chain.find_consolidation_3_hops(wasabi2_events, start, end)\n",
    "# %time res_w1 = chain.find_consolidation_3_hops(wasabi_events, 0, len(chain))\n",
    "%time res_w2 = chain.find_consolidation_3_hops(wasabi2_events, 0, len(chain))\n",
    "# %time res_wh = chain.find_consolidation_3_hops(whirlpool_events, 0, len(chain))\n",
    "\n",
    "# %time res = chain.find_consolidation_3_hops(whirlpool_events, 774513, 778584)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea94e03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      3\u001b[0m max_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tx, outputs \u001b[38;5;129;01min\u001b[39;00m \u001b[43mres\u001b[49m:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m out, val \u001b[38;5;129;01min\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m>\u001b[39m max_outputs[\u001b[38;5;241m2\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "max_outputs = \"\", \"\", 0\n",
    "            \n",
    "for tx, outputs in res:\n",
    "    for out, val in outputs.items():\n",
    "        if val > max_outputs[2]:\n",
    "            max_outputs = out, tx, val\n",
    "\n",
    "print(len(res))\n",
    "print(max_outputs)\n",
    "tx = chain.tx_with_hash(max_outputs[0])\n",
    "coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "tx_start = chain.tx_with_hash(max_outputs[1])\n",
    "\n",
    "coinomon_data = coinomon_client.get_address_info(tx_start.outputs[0].address.address_string)\n",
    "coinomon_data[\"data\"].pop(\"firstTx\")\n",
    "coinomon_data[\"data\"].pop(\"lastTx\")\n",
    "print(json.dumps(coinomon_data, indent=4))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aaf131",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tx, _ in res:\n",
    "    tx = chain.tx_with_hash(tx)\n",
    "    coinomon_data = coinomon_client.get_address_info(tx.outputs[0].address.address_string)\n",
    "    if coinomon_data[\"data\"][\"alarms\"]:\n",
    "        print(coinomon_data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90d58e",
   "metadata": {},
   "source": [
    "# Analysis over the coinjoins (friends do not pay)\n",
    "**(petrs request)**\n",
    "\n",
    "- export all transactions `X`:\n",
    "    - X is 2 hops away from a ww2 coinjoin\n",
    "    - all inputs to X are from a WW2 coinjoin\n",
    "- output the same as `wasabi2_events.json`\n",
    "Structure:\n",
    "- txid\n",
    "    - txid\n",
    "    - block_index\n",
    "    - broadcast_time\n",
    "    - inputs\n",
    "        - input number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - outputs\n",
    "        - output number\n",
    "            - value\n",
    "            - wallet_name\n",
    "            - mix_event_type\n",
    "    - num_inputs\n",
    "    - num_outputs\n",
    "    \n",
    "# !!!!!!! OUTPUT IS SPENT IN SPENDING TX !!!!!!!\n",
    "# !!!!!!! INPUT WAS SPENT IN SPENT TX !!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edb702bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "WW1\n",
      "253d7fce540ec3cfa3acc34704d0e23719beb4e3b9b577113b55f179a0f44208\n",
      "1\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "tx = chain.tx_with_hash(\"253d7fce540ec3cfa3acc34704d0e23719beb4e3b9b577113b55f179a0f44208\")\n",
    "print(tx.is_wasabi1_conjoin)\n",
    "# for o in tx.outputs:\n",
    "#     print(o.address.address_string == 'bc1qs604c7jv6amk4cxqlnvuxv26hv3e48cds4m0ew')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e70b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "# outputs list of transactions where the condition holds\n",
    "def find_friends_do_not_pay_txes(tx: blocksci.Tx, **kwargs) -> List[str]:\n",
    "    ww2_events = kwargs[\"ww2_events\"]\n",
    "    result = []\n",
    "    for out1 in tx.outputs:\n",
    "        if not out1.is_spent:\n",
    "            continue\n",
    "        \n",
    "        for out2 in out1.spending_tx.outputs:\n",
    "            if not out2.is_spent:\n",
    "                continue\n",
    "            \n",
    "            if not out2.spending_tx.hash in ww2_events:\n",
    "                continue\n",
    "            \n",
    "            curr = out2.spending_tx\n",
    "            for inp in curr.inputs:\n",
    "                if str(inp.spent_tx.hash) not in ww2_events:\n",
    "                    break\n",
    "            else:\n",
    "                result.append(str(curr.hash))\n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "%time friends = chain.map_spliterator(map_func=find_friends_do_not_pay_txes, keys=list(wasabi2_events.keys()), data_directory=str(parser_data_directory),ww2_events=wasabi2_events,workers=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ae25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start, stop = get_block_height_range('2023-01-01', '2023-02-01')\n",
    "%time friends = chain.find_friends_who_dont_pay(keys=wasabi2_events, start=0, stop=len(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385d579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "\n",
    "def process_inputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    inp: blocksci.Input\n",
    "    for inp in tx.inputs:\n",
    "        spent_tx: blocksci.Tx = inp.spent_tx\n",
    "        imm = {\n",
    "            str(inp.index): {\n",
    "                \"value\": inp.value,\n",
    "                \"wallet_name\": inp.address,\n",
    "                \"is_ww2_coinjoin\": str(inp.spent_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "counter = 0\n",
    "\n",
    "def process_outputs(tx: blocksci.Tx) -> List[Dict[str, Any]]:\n",
    "    res = []\n",
    "    out: blocksci.Output\n",
    "    for out in tx.outputs:\n",
    "        imm = {\n",
    "            str(out.index): {\n",
    "                \"value\": out.value,\n",
    "                \"wallet_name\": out.address,\n",
    "                \"is_ww2_coinjoin\": out.is_spent and str(out.spending_tx.hash) in wasabi2_events\n",
    "            }\n",
    "        }\n",
    "        res.append(imm)\n",
    "        \n",
    "    return res\n",
    "\n",
    "\n",
    "def fill_json_info(tx: blocksci.Tx) -> Dict[str, Any]:\n",
    "    return {\n",
    "        \"txid\": str(tx.hash),\n",
    "        \"block_index\": str(tx.block_height),\n",
    "        \"broadcast_time\": tx.block_time,\n",
    "        \"num_inputs\": tx.input_count,\n",
    "        \"num_outputs\": tx.output_count,\n",
    "        \"inputs\": process_inputs(tx),\n",
    "        \"outputs\": process_outputs(tx),\n",
    "    }\n",
    "\n",
    "result = {}\n",
    "\n",
    "for tx_id in friends:\n",
    "    tx = chain.tx_with_hash(tx_id)\n",
    "    result[tx_id] = fill_json_info(tx)\n",
    "    \n",
    "print(len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc243d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47min 49s, sys: 1h 39min 23s, total: 2h 27min 13s\n",
      "Wall time: 43.7 s\n",
      "CPU times: user 1h 14min 31s, sys: 5.2 s, total: 1h 14min 36s\n",
      "Wall time: 25.6 s\n",
      "CPU times: user 1h 1min 55s, sys: 1.44 s, total: 1h 1min 56s\n",
      "Wall time: 25.5 s\n",
      "CPU times: user 1h 15min 17s, sys: 3.7 s, total: 1h 15min 21s\n",
      "Wall time: 27.6 s\n",
      "CPU times: user 52min 57s, sys: 3.6 s, total: 53min\n",
      "Wall time: 17.7 s\n",
      "CPU times: user 53min 22s, sys: 3.95 s, total: 53min 26s\n",
      "Wall time: 18.2 s\n"
     ]
    }
   ],
   "source": [
    "strict = False\n",
    "%time w2_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, whirlpool_events, strict)\n",
    "%time wp_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi2_events, strict)\n",
    "%time w1_wp = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, whirlpool_events, strict)\n",
    "%time wp_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), whirlpool_events, wasabi_events, strict)\n",
    "%time w2_w1 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi2_events, wasabi_events, strict)\n",
    "%time w1_w2 = chain.find_traverses_in_coinjoin_flows(0, len(chain), wasabi_events, wasabi2_events, strict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34229a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wasabi2 -> Whirlpool Total length: 1194, case 1: 42, case 2: 1152\n",
      "Wasabi2 -> Whirlpool\n",
      "\tCASE 1 HOP: total: 2834782749 satoshis / 28.35 BTC\n",
      "\tCASE 2 HOPS: total: 51379000302 satoshis / 513.79 BTC\n",
      "Whirlpool -> Wasabi2 Total length: 3198, case 1: 1549, case 2: 1649\n",
      "Whirlpool -> Wasabi2\n",
      "\tCASE 1 HOP: total: 22507026905 satoshis / 225.07 BTC\n",
      "\tCASE 2 HOPS: total: 9515463713 satoshis / 95.15 BTC\n",
      "Wasabi1 -> Whirlpool Total length: 3183, case 1: 28, case 2: 3155\n",
      "Wasabi1 -> Whirlpool\n",
      "\tCASE 1 HOP: total: 265722659 satoshis / 2.66 BTC\n",
      "\tCASE 2 HOPS: total: 95272290375 satoshis / 952.72 BTC\n",
      "Whirlpool -> Wasabi1 Total length: 1660, case 1: 608, case 2: 1052\n",
      "Whirlpool -> Wasabi1\n",
      "\tCASE 1 HOP: total: 8938035036 satoshis / 89.38 BTC\n",
      "\tCASE 2 HOPS: total: 10341313441 satoshis / 103.41 BTC\n",
      "Wasabi2 -> Wasabi1 Total length: 653, case 1: 394, case 2: 259\n",
      "Wasabi2 -> Wasabi1\n",
      "\tCASE 1 HOP: total: 4108480212 satoshis / 41.08 BTC\n",
      "\tCASE 2 HOPS: total: 4218390033 satoshis / 42.18 BTC\n",
      "Wasabi -> Wasabi2 Total length: 7212, case 1: 2861, case 2: 4351\n",
      "Wasabi -> Wasabi2\n",
      "\tCASE 1 HOP: total: 56821126780 satoshis / 568.21 BTC\n",
      "\tCASE 2 HOPS: total: 75837318256 satoshis / 758.37 BTC\n",
      "5482\n"
     ]
    }
   ],
   "source": [
    "labels = [\"Wasabi2 -> Whirlpool\", \"Whirlpool -> Wasabi2\", \"Wasabi1 -> Whirlpool\", \"Whirlpool -> Wasabi1\", \"Wasabi2 -> Wasabi1\", \"Wasabi -> Wasabi2\"]\n",
    "results = [w2_wp, wp_w2, w1_wp, wp_w1, w2_w1, w1_w2]\n",
    "\n",
    "def is_case_1(x):\n",
    "    return len(x[4]) == 0\n",
    "sums = 0\n",
    "\n",
    "for label, result in zip(labels, results):\n",
    "    print(label, f\"Total length: {len(result)}, case 1: {len(list(filter(is_case_1, result)))}, case 2: {len(list(filter(lambda x: not is_case_1(x) , result)))}\")\n",
    "    satoshis_case1 = sum(map(lambda x: x[3], filter(lambda x: is_case_1(x), result)))\n",
    "    satoshis_case2 = sum(map(lambda x: x[3], filter(lambda x: not is_case_1(x), result)))\n",
    "    print(label)\n",
    "    print(f\"\\tCASE 1 HOP: total: {satoshis_case1} satoshis / {round(satoshis_case1 / 100000000, 2)} BTC\")\n",
    "    print(f\"\\tCASE 2 HOPS: total: {satoshis_case2} satoshis / {round(satoshis_case2 / 100000000, 2)} BTC\")\n",
    "    \n",
    "    sums += len(list(filter(lambda x: is_case_1(x), result)))\n",
    "    \n",
    "print(sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "463d134a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1194\n",
      "2743\n",
      "5926\n",
      "6534\n",
      "6928\n",
      "9789\n"
     ]
    }
   ],
   "source": [
    "output_json = []\n",
    "\n",
    "label_result_mapping = {x: set(map(lambda z: z[0], filter(lambda z: len(z[4]) == 0, y))) for x, y in zip(labels, results)}\n",
    "\n",
    "for label, value in zip(labels, results):\n",
    "# for label, values in mixed.items():\n",
    "#     print(label)\n",
    "    for txid, in_cjs, out_cjs, value, pairs in value:\n",
    "#     for txid in values:\n",
    "        if len(pairs) != 0 and '-> Whirlpool' in label:\n",
    "            pass\n",
    "        elif len(pairs) != 0:\n",
    "            continue\n",
    "            \n",
    "#         if txid in label_result_mapping[label]:\n",
    "#             continue\n",
    "            \n",
    "        tx = chain.tx_with_hash(txid)\n",
    "            \n",
    "        r = {\n",
    "            \"txid\": str(tx.hash),\n",
    "            \"broadcast_time\": tx.block_time.isoformat(),\n",
    "            \"in_cjs\": {x: {'value': y, 'broadcast_time': chain.tx_with_hash(x).block_time.isoformat()} for x, y in in_cjs.items()},\n",
    "            \"out_cjs\": {x: {'value': y, 'broadcast_time': chain.tx_with_hash(x).block_time.isoformat()} for x, y in out_cjs.items()},\n",
    "            \"flow_direction\": label,\n",
    "            \"sats_moved\": value,\n",
    "            \"hop_tx_cj_pairs\": {x: y for x, y in pairs}\n",
    "        }\n",
    "        \n",
    "#         output_json[txid] = values[txid]\n",
    "        output_json.append(r)\n",
    "    print(len(output_json))\n",
    "        \n",
    "        \n",
    "# print(len(output_json))\n",
    "with open('/mnt/dumplings/one_hop_flows.json', 'w') as f:\n",
    "    json.dump(output_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbe94607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'broadcast_time': '2022-10-20 05:27:23.000', 'value': 9920232}\n",
      "Wasabi2 -> Whirlpool 42\n",
      "Whirlpool -> Wasabi2 1549\n",
      "Wasabi1 -> Whirlpool 28\n",
      "Whirlpool -> Wasabi1 608\n",
      "Wasabi2 -> Wasabi1 394\n",
      "Wasabi -> Wasabi2 2861\n",
      "4ac7e9e2e58a220f866c265dd391137d83e35695ba052f8c5fc1307c0811d13a\n",
      "{'broadcast_time': '2023-03-06 01:07:19.000', 'value': 100000}\n",
      "7516d0146d6ca2bdb1dea717d25828537e73b949151338e295cd340b59e91a16\n",
      "{'broadcast_time': '2023-03-03 07:35:06.000', 'value': 1000000}\n",
      "e85266be2372837873e2141e0db363a047c485e6043934c5662aaed3b6211bfd\n",
      "{'broadcast_time': '2023-04-03 23:47:12.000', 'value': 50000000}\n",
      "135\n"
     ]
    }
   ],
   "source": [
    "with open(\"/mnt/dumplings/mix_flows.json\", \"r\") as f:\n",
    "    mixed = json.load(f)\n",
    "    \n",
    "    \n",
    "print(mixed['Wasabi2 -> Wasabi1']['7e2083774fffa386464b5c35184337d6e5e80c8f5022248a63c31a9fe2584ea0'])\n",
    "for i in label_result_mapping:\n",
    "    print(i, len(label_result_mapping[i]))\n",
    "\n",
    "curr = set(map(lambda x: x[0], wp_w2))\n",
    "\n",
    "cnt = 0\n",
    "for i in mixed['Whirlpool -> Wasabi2']:\n",
    "    if i not in curr:\n",
    "        cnt += 1\n",
    "        if i not in wasabi2_events:\n",
    "            print(i)\n",
    "            print(mixed['Whirlpool -> Wasabi2'][i])\n",
    "        \n",
    "print(cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8a2c8d",
   "metadata": {},
   "source": [
    "# Real consolidation analysis\n",
    "\n",
    "### Consolidation types\n",
    "1. only 1 output -> certain consolidation\n",
    "2. 1-5 big outputs, more small outputs\n",
    "3. outputs that in dollars make a nice round number\n",
    "\n",
    "\n",
    "### TODO\n",
    "- [x] first get all cjs and their certain 1-hop consolidations\n",
    "- [x] then make it so we can go more hops (while loops)\n",
    "- [x] then then experiment with other consolidation types and thresholds\n",
    "- [ ] profit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9148a902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 58s, sys: 1.93 s, total: 2min\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%time r = chain.get_coinjoin_consolidations(0, len(chain), 5, \"wp\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2de6e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806776.944629\n"
     ]
    }
   ],
   "source": [
    "print(sum(sum(x.input_value for x in y[\"certain\"]) for _, y in r) / 100000000)\n",
    "# print(50968643318271794 / 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c408ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806776.944629\n"
     ]
    }
   ],
   "source": [
    "print(sum(sum(x.input_value for x in y[\"certain\"]) for _, y in r) / 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6180effc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5339.596 3493.718\n"
     ]
    }
   ],
   "source": [
    "certain = set(x for _, y in r for x in y[\"certain\"])\n",
    "possible = set(x for _, y in r for x in y[\"possible\"])\n",
    "\n",
    "print(sum(x.input_value for x in certain) / 100000000, sum(x.input_value for x in possible) / 100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1abfbc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18690\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "c = list(sorted(certain, key=lambda tx: -tx.input_value))\n",
    "print(len(c))\n",
    "\n",
    "final_output = []\n",
    "    \n",
    "for i in range(1000):\n",
    "    tx: blocksci.Tx\n",
    "    tx = c[i]\n",
    "    out = {\n",
    "        \"hash\": str(tx.hash),\n",
    "        \"mempool_link\": tx.mempool_space_link,\n",
    "        \"block\": tx.block_height,\n",
    "        \"time\": tx.block_time.isoformat(),\n",
    "        \"hops\": 1,\n",
    "        \"value\": tx.input_value,\n",
    "        \"consolidations\": [\n",
    "            {\n",
    "                \"hash\": str(x.spent_tx.hash),\n",
    "                \"mempool_link\": x.spent_tx.mempool_space_link,\n",
    "                \"type\": \"wp\", \n",
    "                \"block\": x.spent_tx.block_height,\n",
    "                \"time\": x.spent_tx.block_time.isoformat(),\n",
    "            }\n",
    "            for x in tx.inputs if x.spent_tx.is_whirlpool_coinjoin\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    final_output.append(out)\n",
    "    \n",
    "with open(\"/mnt/anal/certain_consolidations_wp.json\", \"w\") as f:\n",
    "    json.dump(final_output, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "238d14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tx, <c1, c2, c3...>>\n",
    "\n",
    "mapped_consolidations = list(\n",
    "    map(\n",
    "        lambda pair: (\n",
    "            pair[0],\n",
    "            {\n",
    "                \"certain\": {\n",
    "                    \"sum\": sum(x.input_value for x in pair[1][\"certain\"]),\n",
    "                    \"consolidations\": set(pair[1][\"certain\"]),\n",
    "                },\n",
    "                \"possible\": {\n",
    "                    \"sum\": sum(x.input_value for x in pair[1][\"possible\"]),\n",
    "                    \"consolidations\": set(pair[1][\"possible\"]),\n",
    "                },\n",
    "            }\n",
    "        ),\n",
    "        r\n",
    "    ),\n",
    ")\n",
    "\n",
    "sorted_certain_consolidations = list(sorted(mapped_consolidations, key=lambda x: -x[1][\"certain\"][\"sum\"]))\n",
    "sorted_possible_consolidations = list(sorted(mapped_consolidations, key=lambda x: -x[1][\"possible\"][\"sum\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "474ab3af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sorted_certain_consolidations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m tx: blocksci\u001b[38;5;241m.\u001b[39mTx\n\u001b[1;32m      5\u001b[0m cons: List[blocksci\u001b[38;5;241m.\u001b[39mTx]\n\u001b[0;32m----> 6\u001b[0m tx, cons \u001b[38;5;241m=\u001b[39m \u001b[43msorted_certain_consolidations\u001b[49m[i]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(tx\u001b[38;5;241m.\u001b[39mmempool_space_link)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sorted_certain_consolidations' is not defined"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "for i in range(1):\n",
    "    tx: blocksci.Tx\n",
    "    cons: List[blocksci.Tx]\n",
    "    tx, cons = sorted_certain_consolidations[i]\n",
    "    print('--' * 20)\n",
    "    print(tx.mempool_space_link)\n",
    "    print(cons[\"certain\"][\"sum\"] / 100000000)\n",
    "    for tx in sorted(cons[\"certain\"][\"consolidations\"], key=lambda x: -x.input_value):\n",
    "        print(tx.mempool_space_link)\n",
    "        print(tx.input_value / 100000000)\n",
    "        print(tx.input_count, tx.output_count)\n",
    "        \n",
    "    print('--' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c2dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
